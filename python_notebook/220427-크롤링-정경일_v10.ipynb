{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d29296a",
   "metadata": {},
   "source": [
    "# 필요한 module import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad59592",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jki01\\AppData\\Local\\Temp\\ipykernel_9964\\2207815226.py:10: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "from webob.compat import urlparse\n",
    "from IPython.core.display import display, HTML \n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from tqdm import notebook\n",
    "import math\n",
    "from urllib.request import urlopen\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "pd.options.display.max_colwidth = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cc1772",
   "metadata": {},
   "source": [
    "# URL 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb07e6b8",
   "metadata": {},
   "source": [
    "##### url 과 뉴스 제목을 추출하는 function 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe9b63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def craw():\n",
    "    # dl tag의 newsList class를 크롤링\n",
    "    news = bs.find_all('dl', {'class' : 'newsList'})\n",
    "    news_list = []\n",
    "    news_list_2 = []\n",
    "    title_list = []\n",
    "    title_list_2 = []\n",
    "    url_list = []\n",
    "    url_list_2 = []\n",
    "    \n",
    "    #dl tag의 articleSubject와 articleSummary class 저장\n",
    "    for new in news:\n",
    "        title = new.find_all('dt',{'class' : 'articleSubject'})\n",
    "        title_2 = new.find_all('dd',{'class' : 'articleSubject'})\n",
    "#         date_time = new.find_all('dd',{'class' : 'articleSummary'})\n",
    "        # 첫번 째 articleSubject가 저장된 title 에서 url 추출 \n",
    "        for dt in title:\n",
    "            dd = dt.text[1:-1]\n",
    "            link = dt.find(\"a\")[\"href\"]\n",
    "            article_url = 'https://finance.naver.com/' + link\n",
    "            url_list.append(article_url)\n",
    "            title_list.append(dd)\n",
    "        # 두번 째 articleSubject가 저장된 title_2에서 news 제목 추출\n",
    "        for oo in title_2:\n",
    "            article_title = oo.text[1:-1]\n",
    "            link = oo.find(\"a\")[\"href\"]\n",
    "            article_url = 'https://finance.naver.com/' + link\n",
    "            text_list = [article_title, article_url]\n",
    "            url_list_2.append(article_url)\n",
    "            title_list_2.append(article_title)\n",
    "        # 추출한 요소들을 list 형태로 저장해서 url_result 와 title_result로 return\n",
    "        url_result = [url_list, url_list_2]\n",
    "        title_result = [title_list, title_list_2]\n",
    "    return url_result, title_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfd5124",
   "metadata": {},
   "source": [
    "##### 검색 keyword | 시작날짜 | 종료날짜 설정 후 해당하는 url과 뉴스 제목 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ca6028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 검색, 시작날짜, 종료날짜 변수로 저장\n",
    "search = \"삼성전자\"\n",
    "stDateStart= \"2017-01-01\"\n",
    "stDateEnd= \"2022-04-26\"\n",
    "#검색어 인코딩(아스키 오류를 방지하기 위해)\n",
    "euc_data = search.encode('euc-kr')\n",
    "tmp = str(euc_data).replace(\"\\\\x\",\"%\")[2:-1]\n",
    "#여러 페이지 크롤링\n",
    "page = 1\n",
    "url = f\"https://finance.naver.com/news/news_search.naver?rcdate=1&q={tmp}&x=0&y=0&sm=all.basic&pd=4&stDateStart={stDateStart}&stDateEnd={stDateEnd}\"\n",
    "req = requests.get(url)\n",
    "bs = BeautifulSoup(req.content, \"html.parser\")\n",
    "strong = bs.find_all('strong')\n",
    "article_num = strong[-3]\n",
    "article_num = int(re.sub(r\"[^a-zA-Z0-9]\",\"\",article_num.text))\n",
    "i = math.ceil(article_num/20)\n",
    "tmp3 = []\n",
    "print(article_num) # 검색 했을 때 총 몇개의 기사가 검색되는지 표시\n",
    "i += 1\n",
    "for page in tqdm(range(1,i)):\n",
    "    url = f\"https://finance.naver.com/news/news_search.nhn?rcdate=&q={tmp}\\&x=5&y=7&sm=all.basic&pd=1&{stDateStart}&{stDateEnd}&page={page}\"\n",
    "    req = requests.get(url)\n",
    "    bs = BeautifulSoup(req.content, \"html.parser\")\n",
    "    # 기사 갯수 파악 후 for문 범위 정하기\n",
    "    url_list, title_list = craw()\n",
    "    url_list = [y for x in url_list for y in x]\n",
    "    title_list = [y for x in title_list for y in x]\n",
    "    for j in range(len(url_list)):\n",
    "        tmp2 = []\n",
    "        tmp2.append(url_list[j])\n",
    "        tmp2.append(title_list[j])\n",
    "        tmp3.append(tmp2)\n",
    "#     print(url)\n",
    "tmp3 = pd.DataFrame(tmp3)\n",
    "tmp3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1688bc2b",
   "metadata": {},
   "source": [
    "##### 한 페이지당 뉴스가 20개씩 검색 됩니다.\n",
    "##### 크롤링한 페이지가 잘못될 경우 추출한 url 20개가 삼성전자가 아닌 ΜοΦΚάϋάΎ, ЛяМКРќРк, їпЉЇјьјЏ로 잘못 추출되었습니다.\n",
    "##### 아래의 코드로 어떤 것들이 잘못되었는지 전체적으로 확인 가능 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c16e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추출한 url 을 20개씩 출력\n",
    "# for i in tqdm(range(int(len(tmp3) / 20))):\n",
    "#     print(tmp3[0][i*20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135ecdf9",
   "metadata": {},
   "source": [
    "##### 잘못된 url과 올바른 url을 구분해서 변수에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b545f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_error_url = test_samples[test_samples['0'].str.contains('삼성전자') == True]\n",
    "error_url = test_samples[test_samples['0'].str.contains('삼성전자') == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc3fff",
   "metadata": {},
   "source": [
    "##### 잘못된 url을 '삼성전자'로 올바르게 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d9de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대략적으로 어떻게 오류가 났는지 확인 할 수 있는 코드입니다.\n",
    "# error_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error_url에서 'ЛяМКРќРк'을 '삼성전자'로 변경후 error_v1에 저장 \n",
    "error_v1 = []\n",
    "for i in error_url['0']:\n",
    "    error_list = i.replace('ЛяМКРќРк','삼성전자')\n",
    "    error_v1.append(error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a49df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error_v1 'їпЉЇјьјЏ'을 '삼성전자'로 변경후 error_v2에 저장 \n",
    "error_v2 = []\n",
    "for i in error_v1:\n",
    "    error_list = i.replace('їпЉЇјьјЏ', '삼성전자')\n",
    "    error_v2.append(error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66665275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error_v2 'ΜοΦΚάϋάΎ'을 '삼성전자'로 변경후 error_v3에 저장 \n",
    "error_v3 = []\n",
    "for i in error_v2:\n",
    "    error_list = i.replace('ΜοΦΚάϋάΎ', '삼성전자')\n",
    "    error_v3.append(error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61bbaec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 삼성전자가 안들어간 url이 있는지 다시 확인해 줍니다.\n",
    "err_check = pd.DataFrame(error_v3)\n",
    "err_check[err_check[0].str.contains('삼성전자') == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d35ce94",
   "metadata": {},
   "source": [
    "##### 수정된 url과 올바른 url을 다시 합쳐줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07327ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "samsung_url = pd.concat([n_error_url['0'], err_check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332d78c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samsung_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a0f5ab",
   "metadata": {},
   "source": [
    "##### 추출한 data를 CSV 형태로 저장 (아스키 오류를 방지하기 위해)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "samsung_url.to_csv('samsung_url.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = pd.read_csv('samsung_url.csv', index_col=0, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12faedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawling = test_samples['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ca9a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# url 데이터에 손실이 있는지 확인 합니다.\n",
    "print(crawling)\n",
    "print(len(crawling))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661477b",
   "metadata": {},
   "source": [
    "# 뉴스 내용 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2bc19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 초기화\n",
    "test = []\n",
    "test_final = []\n",
    "\n",
    "# 삼성전자 encoding\n",
    "search='삼성전자'\n",
    "search_euc = search.encode('euc-kr')\n",
    "search_ = str(search_euc).replace(\"\\\\x\",\"%\")[2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c8d22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(crawling):\n",
    "    i = re.sub(r'[ㄱ-ㅣ가-힣]',search_, i)\n",
    "    page = urlopen(i)\n",
    "    soup = BeautifulSoup(page, 'html.parser', from_encoding='cp-949')\n",
    "    news_contents = soup.find('div', id='content', class_='articleCont')\n",
    "    news_data = soup.find('span', class_ = 'article_date')\n",
    "    \n",
    "    contents_list = list(news_contents)[2:-2]\n",
    "    contents_list_to_string = ' '.join(str(c) for c in contents_list)\n",
    "    contents_list_to_string_strip = contents_list_to_string.strip(' \\t\\n\\r' )\n",
    "    contents_list_to_string_strip_pattern = re.sub(pattern = '<[^>]*>', repl = '', string = contents_list_to_string)\n",
    "    \n",
    "    test=[]\n",
    "    test.append(contents_list_to_string_strip_pattern)\n",
    "    test.append(news_data.text)\n",
    "    test_final.append(test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab33aeb",
   "metadata": {},
   "source": [
    "##### 최종 파일을 DataFrame으로 만든 후 CSV 형태로 저장 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911dd7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "samsung_news = pd.DataFrame(test_final)\n",
    "samsung_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe491d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "samsung_news.to_csv('samsung_news.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BKST",
   "language": "python",
   "name": "bkst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
