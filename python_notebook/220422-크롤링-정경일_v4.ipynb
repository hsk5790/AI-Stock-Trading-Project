{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad59592",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gyo12\\AppData\\Local\\Temp\\ipykernel_9392\\3500952992.py:9: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "from webob.compat import urlparse\n",
    "from IPython.core.display import display, HTML \n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from tqdm import notebook\n",
    "import math\n",
    "from urllib.request import urlopen\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "pd.options.display.max_colwidth = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe9b63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def craw():\n",
    "    \n",
    "    news = bs.find_all('dl', {'class' : 'newsList'})\n",
    "    \n",
    "    news_list = []\n",
    "    news_list_2 = []\n",
    "    title_list = []\n",
    "    title_list_2 = []\n",
    "    url_list = []\n",
    "    url_list_2 = []\n",
    "    \n",
    "    for new in news:\n",
    "        title = new.find_all('dt',{'class' : 'articleSubject'})\n",
    "        title_2 = new.find_all('dd',{'class' : 'articleSubject'})\n",
    "        date_time = new.find_all('dd',{'class' : 'articleSummary'})\n",
    "    \n",
    "        for dt in title:\n",
    "            dd = dt.text[1:-1]\n",
    "            link = dt.find(\"a\")[\"href\"]\n",
    "            article_url = 'https://finance.naver.com/' + link\n",
    "\n",
    "#             text_list = [dd, article_url]\n",
    "            url_list.append(article_url)\n",
    "            title_list.append(dd)\n",
    "#             news_list.append(text_list)\n",
    "\n",
    "\n",
    "        for oo in title_2:\n",
    "    #         title.append(new.text[1:-1])\n",
    "            article_title = oo.text[1:-1]\n",
    "            link = oo.find(\"a\")[\"href\"]\n",
    "            article_url = 'https://finance.naver.com/' + link\n",
    "\n",
    "            text_list = [article_title, article_url]\n",
    "            url_list_2.append(article_url)\n",
    "            title_list_2.append(article_title)\n",
    "#             news_list_2.append(text_list) \n",
    "        \n",
    "        url_result = [url_list, url_list_2]\n",
    "        title_result = [title_list, title_list_2]\n",
    "    return url_result, title_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ca6028",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6920\n"
     ]
    }
   ],
   "source": [
    "# 검색, 시작날짜, 종료날짜\n",
    "search = \"변기\"\n",
    "stDateStart= \"2021-06-20\"\n",
    "stDateEnd= \"2021-07-20\"\n",
    "#검색어 인코딩\n",
    "euc_data = search.encode('euc-kr')\n",
    "tmp = str(euc_data).replace(\"\\\\x\",\"%\")[2:-1]\n",
    "\n",
    "#여러 페이지 크롤링\n",
    "hh = []\n",
    "page = 1\n",
    "url = f\"https://finance.naver.com/news/news_search.nhn?rcdate=&q={tmp}\\\n",
    "    &x=5&y=7&sm=all.basic&pd=1&{stDateStart}&{stDateEnd}&page={page}\"\n",
    "\n",
    "req = requests.get(url)\n",
    "bs = BeautifulSoup(req.content, \"html.parser\")\n",
    "\n",
    "strong = bs.find_all('strong')\n",
    "article_num = strong[-3]\n",
    "article_num = int(re.sub(r\"[^a-zA-Z0-9]\",\"\",article_num.text))\n",
    "i = math.ceil(article_num/20)\n",
    "\n",
    "tmp3 = []\n",
    "print(article_num)\n",
    "i += 1\n",
    "\n",
    "for page in range(1,i):\n",
    "    url = f\"https://finance.naver.com/news/news_search.nhn?rcdate=&q={tmp}\\\n",
    "        &x=5&y=7&sm=all.basic&pd=1&{stDateStart}&{stDateEnd}&page={page}\"\n",
    "    \n",
    "    req = requests.get(url)\n",
    "    bs = BeautifulSoup(req.content, \"html.parser\")\n",
    "    \n",
    "    # 기사 갯수 파악 후 for문 범위 정하기\n",
    "    \n",
    "    \n",
    "    url_list, title_list = craw()\n",
    "\n",
    "    url_list = [y for x in url_list for y in x]\n",
    "    title_list = [y for x in title_list for y in x]\n",
    "    \n",
    "\n",
    "    for j in range(len(url_list)):\n",
    "        tmp2 = []\n",
    "        tmp2.append(url_list[j])\n",
    "        tmp2.append(title_list[j])\n",
    "        tmp3.append(tmp2)\n",
    "#     print(url)\n",
    "tmp3 = pd.DataFrame(tmp3)\n",
    "tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp3.to_csv('news_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = pd.read_csv('news_sample.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12faedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawling = test_samples['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ab5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "test_final = []\n",
    "bk='변기'\n",
    "euc_data = bk.encode('euc-kr')\n",
    "bk2 = str(euc_data).replace(\"\\\\x\",\"%\")[2:-1]\n",
    "for i in tqdm(crawling):\n",
    "    \n",
    "#     page = urlopen(f'https://finance.naver.com//news/news_read.naver?article_id=0000356944&office_id=448&mode=search&query={bk2}&page=1')\n",
    "#     soup = BeautifulSoup(page, 'html.parser', from_encoding='cp-949')\n",
    "#     # a_v1 = soup.find('div', class_='articleCont')\n",
    "#     a_v1 = soup.find('div', id='content', class_='articleCont')\n",
    "#     b_v1 = soup.find('span', class_ = 'article_date')\n",
    "    i = re.sub(r'[ㄱ-ㅣ가-힣]',bk2, i)\n",
    "#     print(i)\n",
    "    page = urlopen(i)\n",
    "    soup = BeautifulSoup(page, 'html.parser', from_encoding='cp-949')\n",
    "#     print(soup)\n",
    "    \n",
    "    a_v1 = soup.find('div', id='content', class_='articleCont')\n",
    "#     print(a_v1)\n",
    "    \n",
    "    b_v1 = soup.find('span', class_ = 'article_date')\n",
    "#     print(b_v1)\n",
    "    \n",
    "    test = []\n",
    "    test.append(a_v1.text.strip(' \\t\\n\\r'))\n",
    "    test.append(b_v1.text)\n",
    "    test_final.append(test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911dd7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final = pd.DataFrame(test_final)\n",
    "test_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BKST",
   "language": "python",
   "name": "bkst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
